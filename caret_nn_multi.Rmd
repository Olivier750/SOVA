---
title: "Neural Network Multi-Class"
output:
  html_document:
    df_print: paged
  html_notebook:
    theme: cerulean
number_sections: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Commençons par l'installaion des libraries necessaire pour l'analyser, et chargeons les données par la suite.

```{r }
setwd("E:/Others/Formation Data Science/Projet/Nouveau dossier")
library(caret)
library(doParallel)
library(nnet)
## Les 2 bases de données à utiliser :
## BASE_Multi_train et BASE_Multi_test

BASE_Multi_train<-read.csv(file = "E:/Others/Formation Data Science/Projet/Nouveau dossier/Datasources/BASE_Multi_train.csv",header=TRUE,sep=";")
BASE_Multi_test<-read.csv("E:/Others/Formation Data Science/Projet/Nouveau dossier/Datasources/BASE_Multi_test.csv",header=TRUE,sep=";")

# choisir les covariables 
#"Profil","VITESSE","TYPE_CLASSEMENT","AGE",
#"TYPE_RAYON_COURBE", "TRAIN", "UIC","ANNEE_POSE"
names(BASE_Multi_train)
Data_TRAIN<-BASE_Multi_train[,c(6,7,8,9,13,14,15,17,5)]
str(Data_TRAIN)

# Faire la meme chose pour la base test
Data_TEST<-BASE_Multi_test[,c(6,7,8,9,13,14,15,17,5)]


# on scale les données

maxs <- apply(Data_TRAIN[,c("AGE","VITESSE","ANNEE_POSE")], 2, max) 
mins <- apply(Data_TRAIN[,c("AGE","VITESSE","ANNEE_POSE")], 2, min)
Data_TRAIN <- cbind(Data_TRAIN[,c("Profil","TYPE_CLASSEMENT","TYPE_RAYON_COURBE", "TRAIN", "UIC")],as.data.frame(scale(Data_TRAIN[,c("AGE","VITESSE","ANNEE_POSE")], center = mins, scale = maxs - mins)))
rownames(Data_TRAIN)<-NULL

# Faire la meme chose pour la base test

maxs <- apply(Data_TEST[,c("AGE","VITESSE","ANNEE_POSE")], 2, max) 
mins <- apply(Data_TEST[,c("AGE","VITESSE","ANNEE_POSE")], 2, min)
Data_TEST <- cbind(Data_TEST[,c("Profil","TYPE_CLASSEMENT","TYPE_RAYON_COURBE", "TRAIN", "UIC")],as.data.frame(scale(Data_TEST[,c("AGE","VITESSE","ANNEE_POSE")], center = mins, scale = maxs - mins)))
rownames(Data_TEST)<-NULL


```

 Appliquer la fonction FOLD_func pour la constituion des folds :


```{r}

source("E:/Others/Formation Data Science/Projet/Nouveau dossier/Construction_folds.R")

Data_TRAIN1<-FOLDS_func(Data_TRAIN,1444)
Data_TRAIN2<-FOLDS_func(Data_TRAIN,12)


```
Les données sont propres pour l'utilisation. Passons à la mdélisation avec la fonction train de caret. Cette fonction depend de beaucoup de paramètres : 

```{r}
# Grille des paramètres
NN.Grid <- expand.grid(.size=c(15,20,30), .decay=c(0.1,0.2,0.5))

# comme on va faire un 10-fold repetée 2 fois 
## on choisi les seeds pour les itérations 

set.seed(1444)
seeds <- vector(mode = "list", length = 21)
for(i in 1:20) seeds[[i]] <- sample(1000,9)
seeds[[21]] <- 1 # Pour le dernier model
 
## Remplir les paramètres de la traincontrol

# Commençons par l'indice des observations à prendre
IDX<-vector(mode = "list", length = 20)  
for(i in 1:10) IDX[[i]] <-as.integer(rownames(Data_TRAIN1[which(Data_TRAIN1[,"fold"]==i),]))
for(i in 1:10) IDX[[10+i]] <-as.integer(rownames(Data_TRAIN2[which(Data_TRAIN2[,"fold"]==i),]))

Controle<-trainControl(method = "repeatedcv", 
                       number = 10, # the number of folds
                       repeats = 2,
                       classProbs = TRUE, summaryFunction = multiClassSummary,
                       seeds = seeds,index = IDX)


cl = makeCluster(3)
registerDoParallel(cl)

#Neural Model
MOD_NN<- train(TYPE_CLASSEMENT ~ .,
                  data=Data_TRAIN,
                  method='nnet',
                  maxit = 1000,
                  linout = FALSE,
                  trControl = Controle,
                  tuneGrid = NN.Grid,
                  metric = "Accuracy",
                  allowParallel = TRUE)

stopCluster(cl)
remove(cl)
registerDoSEQ()

MOD_NN


```

La metric choisie est le taux de l'accuracy qui atteint 71%.
Tout en choisions les indices IDX fixés auparavant.
Avant de passer à la prévision, essayons de varier le modèle avec autres paramètres.
Le graphique donne : 
```{r fig.height=7, fig.width=10}
plot(MOD_NN, metric = "Accuracy")
```

```{r}
Controle_bis<-trainControl(method = "repeatedcv", 
                       number = 10, # the number of folds
                       repeats = 2,
                       classProbs = TRUE, summaryFunction = multiClassSummary,
                       seeds = seeds)


cl = makeCluster(3)
registerDoParallel(cl)

#Neural Model
MOD_NN_bis<- train(TYPE_CLASSEMENT ~ .,
                  data=Data_TRAIN,
                  method='nnet',
                  maxit = 1000,
                  linout = FALSE,
                  trControl = Controle_bis,
                  tuneGrid = NN.Grid,
                  metric = "Accuracy",
                  allowParallel = TRUE)

stopCluster(cl)
remove(cl)
registerDoSEQ()

MOD_NN_bis



```
Le graphique donne :
```{r,fig.height=7, fig.width=10}
plot(MOD_NN_bis, metric = "Accuracy")

```
Passons maintenant à la prévision


```{r}

NN_Predictions <-predict(MOD_NN_bis, Data_TEST,type = "prob")
library(nnet)
## faisons varier le seuil k et regardons le taux d'erreur
P<-seq(0,1,0.05)
ERREUR<-c()
for (i in 1:length(P)){
Confusion<-(NN_Predictions>P[i])+0
X<-apply(Confusion,1,which.max)
X[which(X==1)]<-"O"
X[which(X==2)]<-"X1"
X[which(X==3)]<-"X2"
X<-as.factor(as.matrix(as.factor(X)))
Data_TEST$TYPE_CLASSEMENT<-as.factor(Data_TEST$TYPE_CLASSEMENT)
levels(X)<-levels(Data_TEST$TYPE_CLASSEMENT)
MAT<-confusionMatrix(X, Data_TEST$TYPE_CLASSEMENT)$table
ERR<-(MAT[1,2]+MAT[1,3]+MAT[2,1]+MAT[2,3]+MAT[3,1]+MAT[3,2])/nrow(Data_TEST)
ERREUR=c(ERREUR,ERR)
}
ERREUR
plot(P,ERREUR,type='p',col="blue")
cbind(P,ERREUR)


```
```{r}
Confusion<-(NN_Predictions>0.45)+0
X<-apply(Confusion,1,which.max)
X[which(X==1)]<-"O"
X[which(X==2)]<-"X1"
X[which(X==3)]<-"X2"
X<-as.factor(as.matrix(as.factor(X)))
Data_TEST$TYPE_CLASSEMENT<-as.factor(Data_TEST$TYPE_CLASSEMENT)
levels(X)<-levels(Data_TEST$TYPE_CLASSEMENT)
MAT<-confusionMatrix(X, Data_TEST$TYPE_CLASSEMENT)$table
ERR<-(MAT[1,2]+MAT[1,3]+MAT[2,1]+MAT[2,3]+MAT[3,1]+MAT[3,2])/nrow(Data_TEST)
MAT
c('Erreur ',ERR)
```
